{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fcc2e3befa134710ae13c1fd845279db",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import os\n",
    "from datetime import datetime\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import udf, col, lit, date_format\n",
    "from pyspark.sql.functions import year, month, dayofmonth, hour, weekofyear\n",
    "from pyspark.sql import types as time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dea712f193cf4d8c9655b7956db43202",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "spark = SparkSession.builder\\\n",
    "                     .config(\"spark.jars.packages\",\"org.apache.hadoop:hadoop-aws:2.7.0\")\\\n",
    "                     .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c96241941113496e87d3bf1421e35df4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'Path does not exist: s3a://udacity-dend/song_data/T/I/L/*.json;'\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib/spark/python/lib/pyspark.zip/pyspark/sql/readwriter.py\", line 274, in json\n",
      "    return self._df(self._jreader.json(self._spark._sc._jvm.PythonUtils.toSeq(path)))\n",
      "  File \"/usr/lib/spark/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py\", line 1257, in __call__\n",
      "    answer, self.gateway_client, self.target_id, self.name)\n",
      "  File \"/usr/lib/spark/python/lib/pyspark.zip/pyspark/sql/utils.py\", line 69, in deco\n",
      "    raise AnalysisException(s.split(': ', 1)[1], stackTrace)\n",
      "pyspark.sql.utils.AnalysisException: 'Path does not exist: s3a://udacity-dend/song_data/T/I/L/*.json;'\n",
      "\n"
     ]
    }
   ],
   "source": [
    "songs_stage = spark.read.json(\"s3a://udacity-dend/song_data/A/A/B/*.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "49f05d83df014277b343254d12347845",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+---------------+--------------------+----------------+---------------+---------+---------+------------------+--------------------+----+\n",
      "|         artist_id|artist_latitude|     artist_location|artist_longitude|    artist_name| duration|num_songs|           song_id|               title|year|\n",
      "+------------------+---------------+--------------------+----------------+---------------+---------+---------+------------------+--------------------+----+\n",
      "|ARKIQCZ1187B9A7C7C|       52.23974|Northampton, Nort...|        -0.88576|        Bauhaus|248.65914|        1|SOSIJKW12A8C1330E3|A God In An Alcov...|   0|\n",
      "|ARI9DQS1187B995625|       44.93746|  Chippewa Falls, WI|       -91.39251|    Judy Henske|139.78077|        1|SODVVEL12A6D4F9EA0|Good Old Wagon (L...|1964|\n",
      "|AR2L9A61187B9ADDBC|       52.51607|     Berlin, Germany|        13.37698|Tangerine Dream|492.30322|        1|SOKPKMV12A8C14125E|Catwalk (Black In...|1995|\n",
      "|ARQTC851187B9B03AF|       39.08166|       Rockville, MD|       -77.15124|         O.A.R.|263.99302|        1|SOAADAD12A8C13D5B0|One Shot (Album V...|2005|\n",
      "|ARD842G1187B997376|       43.64856|Toronto, Ontario,...|       -79.38533|     Blue Rodeo|491.12771|        1|SOHUOAP12A8AE488E9|            Floating|1987|\n",
      "+------------------+---------------+--------------------+----------------+---------------+---------+---------+------------------+--------------------+----+\n",
      "only showing top 5 rows\n",
      "\n",
      "root\n",
      " |-- artist_id: string (nullable = true)\n",
      " |-- artist_latitude: double (nullable = true)\n",
      " |-- artist_location: string (nullable = true)\n",
      " |-- artist_longitude: double (nullable = true)\n",
      " |-- artist_name: string (nullable = true)\n",
      " |-- duration: double (nullable = true)\n",
      " |-- num_songs: long (nullable = true)\n",
      " |-- song_id: string (nullable = true)\n",
      " |-- title: string (nullable = true)\n",
      " |-- year: long (nullable = true)"
     ]
    }
   ],
   "source": [
    "songs_stage.show(5)\n",
    "songs_stage.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6d00c9d9de9b494989ede92cfe069123",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "name 'desc' is not defined\n",
      "Traceback (most recent call last):\n",
      "NameError: name 'desc' is not defined\n",
      "\n"
     ]
    }
   ],
   "source": [
    "songs_table = songs_stage.select(col('song_id'),\n",
    "                       col('title'),\n",
    "                       col('artist_id'),\n",
    "                       col('year'),\n",
    "                       col('duration')).orderBy('song_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4c60d4b1363a4f87acc27678715ba2db",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+--------------------+------------------+----+---------+\n",
      "|           song_id|               title|         artist_id|year| duration|\n",
      "+------------------+--------------------+------------------+----+---------+\n",
      "|SOAADAD12A8C13D5B0|One Shot (Album V...|ARQTC851187B9B03AF|2005|263.99302|\n",
      "|SOBXHCE12AB018B6AC|     Arms and Armour|ARV6GHH1187B9AED0D|2008|249.41669|\n",
      "|SODVVEL12A6D4F9EA0|Good Old Wagon (L...|ARI9DQS1187B995625|1964|139.78077|\n",
      "+------------------+--------------------+------------------+----+---------+\n",
      "only showing top 3 rows"
     ]
    }
   ],
   "source": [
    "songs_table.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a08396a257ab46f3b27f5fd220a95d26",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "songs_table.write.partitionBy('year', 'artist_id').parquet(\"s3a://spark-output-apk/songs.parquet\", 'overwrite')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "78f7943a041b4a25aaca1fa9313eb9ad",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "artists_table = songs_stage.select(col('artist_id'),\n",
    "                       col('artist_name').alias('name'),\n",
    "                       col('artist_location').alias('location'),\n",
    "                       col('artist_latitude').alias('latitude'),\n",
    "                       col('artist_longitude').alias('longitude')).distinct()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "48d79f030e814d19a58cdd6d2548ba18",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+------------------+------------------+--------+---------+\n",
      "|         artist_id|              name|          location|latitude|longitude|\n",
      "+------------------+------------------+------------------+--------+---------+\n",
      "|ARPIKA31187FB4C233|        The Action|          New York|40.71455|-74.00712|\n",
      "|AR5AA4Q1187FB4CFBD|    Alisha's Attic|                  |    null|     null|\n",
      "|ARV6GHH1187B9AED0D|      Sofia Talvik|            SWEDEN|62.19845| 17.55142|\n",
      "|ARE6D0Y119B8667D9F|Alejandro Santiago|                  |    null|     null|\n",
      "|ARZD4UW1187B9AB3D2|       Latin Lover|NEW YORK, New York|40.71455|-74.00712|\n",
      "+------------------+------------------+------------------+--------+---------+\n",
      "only showing top 5 rows"
     ]
    }
   ],
   "source": [
    "artists_table.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2afdbc3a16274e9d935b099204f86f7f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "artists_table.write.parquet(os.path.join(\"s3a://spark-output-apk/artists.parquet\"), 'overwrite')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ee10b9f28d4b45a682b66a16fa617836",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "log_data_raw = spark.read.json(\"s3a://udacity-dend/log_data/2018/11/2018-11-13-events.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0a4f52f486d647278ae3f7af2e35d9a1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "log_data_stage = log_data_raw.filter(log_data_raw.page == 'NextSong')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "030dc89d8bf84877970b8ee4f92f9309",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+--------------------+\n",
      "|         artist|                song|\n",
      "+---------------+--------------------+\n",
      "|    Dash Berlin|Till The Sky Fall...|\n",
      "|   Brian Hyland|  Sealed With A Kiss|\n",
      "| The Black Keys|          Your Touch|\n",
      "| The Black Keys|     Howlin\u0019 For You|\n",
      "|Balkan Beat Box|   Bulgarian  Chicks|\n",
      "+---------------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "+-----------+-----+\n",
      "|artist_name|title|\n",
      "+-----------+-----+\n",
      "+-----------+-----+"
     ]
    }
   ],
   "source": [
    "log_data_stage.select('artist', 'song').filter(log_data_stage.artist.contains('B')).show(5)\n",
    "songs_stage.select('artist_name', 'title').filter(songs_stage.artist_name.contains('Black Keys')).show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "36b8e4eed84b4e2eb7cddf750400f5fd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+\n",
      "|        artist|\n",
      "+--------------+\n",
      "|            Fu|\n",
      "|  All Time Low|\n",
      "|     Nik & Jay|\n",
      "|Quad City DJ's|\n",
      "|    Hoobastank|\n",
      "+--------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "root\n",
      " |-- artist: string (nullable = true)\n",
      " |-- auth: string (nullable = true)\n",
      " |-- firstName: string (nullable = true)\n",
      " |-- gender: string (nullable = true)\n",
      " |-- itemInSession: long (nullable = true)\n",
      " |-- lastName: string (nullable = true)\n",
      " |-- length: double (nullable = true)\n",
      " |-- level: string (nullable = true)\n",
      " |-- location: string (nullable = true)\n",
      " |-- method: string (nullable = true)\n",
      " |-- page: string (nullable = true)\n",
      " |-- registration: double (nullable = true)\n",
      " |-- sessionId: long (nullable = true)\n",
      " |-- song: string (nullable = true)\n",
      " |-- status: long (nullable = true)\n",
      " |-- ts: long (nullable = true)\n",
      " |-- userAgent: string (nullable = true)\n",
      " |-- userId: string (nullable = true)\n",
      " |-- timestamp: string (nullable = true)\n",
      " |-- date: string (nullable = true)"
     ]
    }
   ],
   "source": [
    "log_data_stage.select('artist').show(5)\n",
    "log_data_stage.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e83c8e6e4ebd4e4b996a6cee9e640de4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "users_table = log_data_stage.select(col('userId').alias('user_id'),\n",
    "                       col('firstName').alias('firt_name'),\n",
    "                       col('lastName').alias('last_name'),\n",
    "                       col('gender'),\n",
    "                       col('level')).distinct()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "012c59d8d38c44ffba86d8f0eab6bccc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----------+---------+------+-----+\n",
      "|user_id| firt_name|last_name|gender|level|\n",
      "+-------+----------+---------+------+-----+\n",
      "|     63|      Ayla|  Johnson|     F| free|\n",
      "|     37|    Jordan|    Hicks|     F| free|\n",
      "|      6|   Cecilia|    Owens|     F| free|\n",
      "|     15|      Lily|     Koch|     F| paid|\n",
      "|     29|Jacqueline|    Lynch|     F| paid|\n",
      "+-------+----------+---------+------+-----+\n",
      "only showing top 5 rows"
     ]
    }
   ],
   "source": [
    "users_table.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ffac37a28fbc431cad76cb10a8388dd0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "users_table.write.parquet(os.path.join(\"s3a://spark-output-apk/users.parquet\"), 'overwrite')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "17db208872af4b00bb8cc624ae1715f1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "get_timestamp = udf(lambda x: datetime.fromtimestamp(x/1000))\n",
    "log_data_stage = log_data_stage.withColumn('timestamp', get_timestamp(log_data_stage.ts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "75de1c699c1f4a58a6f3781e6463b641",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "get_datetime = udf(lambda x: datetime.fromtimestamp(x/1000).strftime('%Y-%m-%d %H:%M:%S'))\n",
    "log_data_stage = log_data_stage.withColumn('date', get_datetime(log_data_stage.ts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6dc886b53cf449aea28f17d10c0cc556",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- artist: string (nullable = true)\n",
      " |-- auth: string (nullable = true)\n",
      " |-- firstName: string (nullable = true)\n",
      " |-- gender: string (nullable = true)\n",
      " |-- itemInSession: long (nullable = true)\n",
      " |-- lastName: string (nullable = true)\n",
      " |-- length: double (nullable = true)\n",
      " |-- level: string (nullable = true)\n",
      " |-- location: string (nullable = true)\n",
      " |-- method: string (nullable = true)\n",
      " |-- page: string (nullable = true)\n",
      " |-- registration: double (nullable = true)\n",
      " |-- sessionId: long (nullable = true)\n",
      " |-- song: string (nullable = true)\n",
      " |-- status: long (nullable = true)\n",
      " |-- ts: long (nullable = true)\n",
      " |-- userAgent: string (nullable = true)\n",
      " |-- userId: string (nullable = true)\n",
      " |-- timestamp: string (nullable = true)\n",
      " |-- date: string (nullable = true)\n",
      "\n",
      "+-------------------+\n",
      "|               date|\n",
      "+-------------------+\n",
      "|2018-11-13 00:40:37|\n",
      "+-------------------+\n",
      "only showing top 1 row"
     ]
    }
   ],
   "source": [
    "log_data_stage.printSchema()\n",
    "log_data_stage.select('date').show(1, truncate=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6e06eb0f36974eb5b4e5a735df04c815",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "time_table = log_data_stage.select(col('date').alias('start_time'),\n",
    "                       hour('date').alias('hour'),\n",
    "                       dayofmonth('date').alias('day'),\n",
    "                       weekofyear('date').alias('week'),\n",
    "                       month('date').alias('month'),\n",
    "                       year('date').alias('year'),\n",
    "                       date_format('date','EEEE').alias('weekday')).distinct().orderBy('start_time')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fce14e48f577443cad2a5a9bf95a99e1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+----+---+----+-----+----+-------+\n",
      "|         start_time|hour|day|week|month|year|weekday|\n",
      "+-------------------+----+---+----+-----+----+-------+\n",
      "|2018-11-13 00:40:37|   0| 13|  46|   11|2018|Tuesday|\n",
      "|2018-11-13 01:12:29|   1| 13|  46|   11|2018|Tuesday|\n",
      "|2018-11-13 03:19:02|   3| 13|  46|   11|2018|Tuesday|\n",
      "|2018-11-13 03:51:52|   3| 13|  46|   11|2018|Tuesday|\n",
      "|2018-11-13 05:00:06|   5| 13|  46|   11|2018|Tuesday|\n",
      "+-------------------+----+---+----+-----+----+-------+\n",
      "only showing top 5 rows"
     ]
    }
   ],
   "source": [
    "time_table.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "668681aefd2f413e9a5a7b60072b9e35",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "An error occurred while calling o2323.parquet.\n",
      ": org.apache.spark.SparkException: Job aborted.\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:198)\n",
      "\tat org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand.run(InsertIntoHadoopFsRelationCommand.scala:172)\n",
      "\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult$lzycompute(commands.scala:104)\n",
      "\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult(commands.scala:102)\n",
      "\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.doExecute(commands.scala:122)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$execute$1(SparkPlan.scala:131)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$executeQuery$1(SparkPlan.scala:156)\n",
      "\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:152)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:127)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.toRdd$lzycompute(QueryExecution.scala:83)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.toRdd(QueryExecution.scala:83)\n",
      "\tat org.apache.spark.sql.DataFrameWriter.$anonfun$runCommand$1(DataFrameWriter.scala:676)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:84)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:165)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:74)\n",
      "\tat org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:676)\n",
      "\tat org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:290)\n",
      "\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:271)\n",
      "\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:229)\n",
      "\tat org.apache.spark.sql.DataFrameWriter.parquet(DataFrameWriter.scala:566)\n",
      "\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n",
      "\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n",
      "\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n",
      "\tat java.lang.reflect.Method.invoke(Method.java:498)\n",
      "\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n",
      "\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n",
      "\tat py4j.Gateway.invoke(Gateway.java:282)\n",
      "\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n",
      "\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n",
      "\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n",
      "\tat java.lang.Thread.run(Thread.java:748)\n",
      "Caused by: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 84.0 failed 4 times, most recent failure: Lost task 0.3 in stage 84.0 (TID 902, ip-172-31-13-223.ec2.internal, executor 23): java.lang.RuntimeException: Failed to run command: /usr/bin/virtualenv -p python3 --system-site-packages virtualenv_application_1613422903861_0005_0\n",
      "\tat org.apache.spark.api.python.VirtualEnvFactory.execCommand(VirtualEnvFactory.scala:120)\n",
      "\tat org.apache.spark.api.python.VirtualEnvFactory.setupVirtualEnv(VirtualEnvFactory.scala:78)\n",
      "\tat org.apache.spark.api.python.PythonWorkerFactory.<init>(PythonWorkerFactory.scala:85)\n",
      "\tat org.apache.spark.SparkEnv.$anonfun$createPythonWorker$1(SparkEnv.scala:118)\n",
      "\tat scala.collection.mutable.HashMap.getOrElseUpdate(HashMap.scala:86)\n",
      "\tat org.apache.spark.SparkEnv.createPythonWorker(SparkEnv.scala:118)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner.compute(PythonRunner.scala:109)\n",
      "\tat org.apache.spark.sql.execution.python.BatchEvalPythonExec.evaluate(BatchEvalPythonExec.scala:77)\n",
      "\tat org.apache.spark.sql.execution.python.EvalPythonExec.$anonfun$doExecute$2(EvalPythonExec.scala:128)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2(RDD.scala:801)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2$adapted(RDD.scala:801)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:288)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:288)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:288)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:55)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:123)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:411)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:748)\n",
      "\n",
      "Driver stacktrace:\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2041)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2029)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2028)\n",
      "\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n",
      "\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n",
      "\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2028)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:966)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:966)\n",
      "\tat scala.Option.foreach(Option.scala:407)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:966)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2262)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2211)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2200)\n",
      "\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:777)\n",
      "\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2061)\n",
      "\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2082)\n",
      "\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2101)\n",
      "\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2126)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$collect$1(RDD.scala:945)\n",
      "\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n",
      "\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n",
      "\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:363)\n",
      "\tat org.apache.spark.rdd.RDD.collect(RDD.scala:944)\n",
      "\tat org.apache.spark.RangePartitioner$.sketch(Partitioner.scala:304)\n",
      "\tat org.apache.spark.RangePartitioner.<init>(Partitioner.scala:171)\n",
      "\tat org.apache.spark.sql.execution.exchange.ShuffleExchangeExec$.prepareShuffleDependency(ShuffleExchangeExec.scala:224)\n",
      "\tat org.apache.spark.sql.execution.exchange.ShuffleExchangeExec.prepareShuffleDependency(ShuffleExchangeExec.scala:92)\n",
      "\tat org.apache.spark.sql.execution.exchange.ShuffleExchangeExec.$anonfun$doExecute$1(ShuffleExchangeExec.scala:128)\n",
      "\tat org.apache.spark.sql.catalyst.errors.package$.attachTree(package.scala:52)\n",
      "\tat org.apache.spark.sql.execution.exchange.ShuffleExchangeExec.doExecute(ShuffleExchangeExec.scala:119)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$execute$1(SparkPlan.scala:131)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$executeQuery$1(SparkPlan.scala:156)\n",
      "\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:152)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:127)\n",
      "\tat org.apache.spark.sql.execution.InputAdapter.inputRDDs(WholeStageCodegenExec.scala:391)\n",
      "\tat org.apache.spark.sql.execution.SortExec.inputRDDs(SortExec.scala:121)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenExec.doExecute(WholeStageCodegenExec.scala:627)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$execute$1(SparkPlan.scala:131)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$executeQuery$1(SparkPlan.scala:156)\n",
      "\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:152)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:127)\n",
      "\tat org.apache.spark.sql.execution.SortExec.doExecute(SortExec.scala:101)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$execute$1(SparkPlan.scala:131)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$executeQuery$1(SparkPlan.scala:156)\n",
      "\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:152)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:127)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:151)\n",
      "\t... 31 more\n",
      "Caused by: java.lang.RuntimeException: Failed to run command: /usr/bin/virtualenv -p python3 --system-site-packages virtualenv_application_1613422903861_0005_0\n",
      "\tat org.apache.spark.api.python.VirtualEnvFactory.execCommand(VirtualEnvFactory.scala:120)\n",
      "\tat org.apache.spark.api.python.VirtualEnvFactory.setupVirtualEnv(VirtualEnvFactory.scala:78)\n",
      "\tat org.apache.spark.api.python.PythonWorkerFactory.<init>(PythonWorkerFactory.scala:85)\n",
      "\tat org.apache.spark.SparkEnv.$anonfun$createPythonWorker$1(SparkEnv.scala:118)\n",
      "\tat scala.collection.mutable.HashMap.getOrElseUpdate(HashMap.scala:86)\n",
      "\tat org.apache.spark.SparkEnv.createPythonWorker(SparkEnv.scala:118)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner.compute(PythonRunner.scala:109)\n",
      "\tat org.apache.spark.sql.execution.python.BatchEvalPythonExec.evaluate(BatchEvalPythonExec.scala:77)\n",
      "\tat org.apache.spark.sql.execution.python.EvalPythonExec.$anonfun$doExecute$2(EvalPythonExec.scala:128)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2(RDD.scala:801)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2$adapted(RDD.scala:801)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:288)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:288)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:288)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:55)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:123)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:411)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\t... 1 more\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib/spark/python/lib/pyspark.zip/pyspark/sql/readwriter.py\", line 843, in parquet\n",
      "    self._jwrite.parquet(path)\n",
      "  File \"/usr/lib/spark/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py\", line 1257, in __call__\n",
      "    answer, self.gateway_client, self.target_id, self.name)\n",
      "  File \"/usr/lib/spark/python/lib/pyspark.zip/pyspark/sql/utils.py\", line 63, in deco\n",
      "    return f(*a, **kw)\n",
      "  File \"/usr/lib/spark/python/lib/py4j-0.10.7-src.zip/py4j/protocol.py\", line 328, in get_return_value\n",
      "    format(target_id, \".\", name), value)\n",
      "py4j.protocol.Py4JJavaError: An error occurred while calling o2323.parquet.\n",
      ": org.apache.spark.SparkException: Job aborted.\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:198)\n",
      "\tat org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand.run(InsertIntoHadoopFsRelationCommand.scala:172)\n",
      "\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult$lzycompute(commands.scala:104)\n",
      "\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult(commands.scala:102)\n",
      "\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.doExecute(commands.scala:122)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$execute$1(SparkPlan.scala:131)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$executeQuery$1(SparkPlan.scala:156)\n",
      "\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:152)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:127)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.toRdd$lzycompute(QueryExecution.scala:83)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.toRdd(QueryExecution.scala:83)\n",
      "\tat org.apache.spark.sql.DataFrameWriter.$anonfun$runCommand$1(DataFrameWriter.scala:676)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:84)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:165)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:74)\n",
      "\tat org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:676)\n",
      "\tat org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:290)\n",
      "\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:271)\n",
      "\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:229)\n",
      "\tat org.apache.spark.sql.DataFrameWriter.parquet(DataFrameWriter.scala:566)\n",
      "\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n",
      "\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n",
      "\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n",
      "\tat java.lang.reflect.Method.invoke(Method.java:498)\n",
      "\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n",
      "\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n",
      "\tat py4j.Gateway.invoke(Gateway.java:282)\n",
      "\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n",
      "\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n",
      "\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n",
      "\tat java.lang.Thread.run(Thread.java:748)\n",
      "Caused by: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 84.0 failed 4 times, most recent failure: Lost task 0.3 in stage 84.0 (TID 902, ip-172-31-13-223.ec2.internal, executor 23): java.lang.RuntimeException: Failed to run command: /usr/bin/virtualenv -p python3 --system-site-packages virtualenv_application_1613422903861_0005_0\n",
      "\tat org.apache.spark.api.python.VirtualEnvFactory.execCommand(VirtualEnvFactory.scala:120)\n",
      "\tat org.apache.spark.api.python.VirtualEnvFactory.setupVirtualEnv(VirtualEnvFactory.scala:78)\n",
      "\tat org.apache.spark.api.python.PythonWorkerFactory.<init>(PythonWorkerFactory.scala:85)\n",
      "\tat org.apache.spark.SparkEnv.$anonfun$createPythonWorker$1(SparkEnv.scala:118)\n",
      "\tat scala.collection.mutable.HashMap.getOrElseUpdate(HashMap.scala:86)\n",
      "\tat org.apache.spark.SparkEnv.createPythonWorker(SparkEnv.scala:118)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner.compute(PythonRunner.scala:109)\n",
      "\tat org.apache.spark.sql.execution.python.BatchEvalPythonExec.evaluate(BatchEvalPythonExec.scala:77)\n",
      "\tat org.apache.spark.sql.execution.python.EvalPythonExec.$anonfun$doExecute$2(EvalPythonExec.scala:128)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2(RDD.scala:801)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2$adapted(RDD.scala:801)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:288)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:288)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:288)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:55)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:123)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:411)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:748)\n",
      "\n",
      "Driver stacktrace:\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2041)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2029)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2028)\n",
      "\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n",
      "\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n",
      "\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2028)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:966)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:966)\n",
      "\tat scala.Option.foreach(Option.scala:407)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:966)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2262)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2211)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2200)\n",
      "\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:777)\n",
      "\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2061)\n",
      "\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2082)\n",
      "\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2101)\n",
      "\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2126)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$collect$1(RDD.scala:945)\n",
      "\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n",
      "\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n",
      "\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:363)\n",
      "\tat org.apache.spark.rdd.RDD.collect(RDD.scala:944)\n",
      "\tat org.apache.spark.RangePartitioner$.sketch(Partitioner.scala:304)\n",
      "\tat org.apache.spark.RangePartitioner.<init>(Partitioner.scala:171)\n",
      "\tat org.apache.spark.sql.execution.exchange.ShuffleExchangeExec$.prepareShuffleDependency(ShuffleExchangeExec.scala:224)\n",
      "\tat org.apache.spark.sql.execution.exchange.ShuffleExchangeExec.prepareShuffleDependency(ShuffleExchangeExec.scala:92)\n",
      "\tat org.apache.spark.sql.execution.exchange.ShuffleExchangeExec.$anonfun$doExecute$1(ShuffleExchangeExec.scala:128)\n",
      "\tat org.apache.spark.sql.catalyst.errors.package$.attachTree(package.scala:52)\n",
      "\tat org.apache.spark.sql.execution.exchange.ShuffleExchangeExec.doExecute(ShuffleExchangeExec.scala:119)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$execute$1(SparkPlan.scala:131)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$executeQuery$1(SparkPlan.scala:156)\n",
      "\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:152)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:127)\n",
      "\tat org.apache.spark.sql.execution.InputAdapter.inputRDDs(WholeStageCodegenExec.scala:391)\n",
      "\tat org.apache.spark.sql.execution.SortExec.inputRDDs(SortExec.scala:121)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenExec.doExecute(WholeStageCodegenExec.scala:627)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$execute$1(SparkPlan.scala:131)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$executeQuery$1(SparkPlan.scala:156)\n",
      "\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:152)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:127)\n",
      "\tat org.apache.spark.sql.execution.SortExec.doExecute(SortExec.scala:101)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$execute$1(SparkPlan.scala:131)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$executeQuery$1(SparkPlan.scala:156)\n",
      "\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:152)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:127)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:151)\n",
      "\t... 31 more\n",
      "Caused by: java.lang.RuntimeException: Failed to run command: /usr/bin/virtualenv -p python3 --system-site-packages virtualenv_application_1613422903861_0005_0\n",
      "\tat org.apache.spark.api.python.VirtualEnvFactory.execCommand(VirtualEnvFactory.scala:120)\n",
      "\tat org.apache.spark.api.python.VirtualEnvFactory.setupVirtualEnv(VirtualEnvFactory.scala:78)\n",
      "\tat org.apache.spark.api.python.PythonWorkerFactory.<init>(PythonWorkerFactory.scala:85)\n",
      "\tat org.apache.spark.SparkEnv.$anonfun$createPythonWorker$1(SparkEnv.scala:118)\n",
      "\tat scala.collection.mutable.HashMap.getOrElseUpdate(HashMap.scala:86)\n",
      "\tat org.apache.spark.SparkEnv.createPythonWorker(SparkEnv.scala:118)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner.compute(PythonRunner.scala:109)\n",
      "\tat org.apache.spark.sql.execution.python.BatchEvalPythonExec.evaluate(BatchEvalPythonExec.scala:77)\n",
      "\tat org.apache.spark.sql.execution.python.EvalPythonExec.$anonfun$doExecute$2(EvalPythonExec.scala:128)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2(RDD.scala:801)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2$adapted(RDD.scala:801)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:288)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:288)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:288)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:55)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:123)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:411)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\t... 1 more\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "time_table.write.partitionBy('year', 'month').parquet(\"s3a://spark-output-apk/time.parquet\", 'overwrite')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3d01ae624ccd4d6fad0641717d462f16",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "songplays_table = log_data_stage.join(\\\n",
    "                                      songs_stage,\\\n",
    "                                      (log_data_stage.artist == songs_stage.artist_name)\\\n",
    "                                      & (log_data_stage.song == songs_stage.title))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0c3cf03d910342469c0dac0bff8b503f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+----+---------+------+-------------+--------+------+-----+--------+------+----+------------+---------+----+------+---+---------+------+---------+----+---------+---------------+---------------+----------------+-----------+--------+---------+-------+-----+----+\n",
      "|artist|auth|firstName|gender|itemInSession|lastName|length|level|location|method|page|registration|sessionId|song|status| ts|userAgent|userId|timestamp|date|artist_id|artist_latitude|artist_location|artist_longitude|artist_name|duration|num_songs|song_id|title|year|\n",
      "+------+----+---------+------+-------------+--------+------+-----+--------+------+----+------------+---------+----+------+---+---------+------+---------+----+---------+---------------+---------------+----------------+-----------+--------+---------+-------+-----+----+\n",
      "+------+----+---------+------+-------------+--------+------+-----+--------+------+----+------------+---------+----+------+---+---------+------+---------+----+---------+---------------+---------------+----------------+-----------+--------+---------+-------+-----+----+"
     ]
    }
   ],
   "source": [
    "songplays_table.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "songplays_table.write.partitionBy('year', 'month').parquet(\"s3a://spark-output-apk/songplays.parquet\", 'overwrite')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PySpark",
   "language": "",
   "name": "pysparkkernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "python",
    "version": 2
   },
   "mimetype": "text/x-python",
   "name": "pyspark",
   "pygments_lexer": "python2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
